# Project Context: Personal Assistant

This document provides an overview of the personal-assistant project architecture and key components.

## Project Overview

The personal-assistant is a research-focused question answering system that uses Retrieval Augmented Generation (RAG) to provide detailed, sourced answers to user queries. The system generates a tree of related questions and answers to provide comprehensive coverage of a topic.

## Key Components

### 1. Lambda Function

The main entry point for the application is a serverless AWS Lambda function that:
- Receives user queries
- Processes them through the RAG engine
- Returns structured responses with answers and sources
- Generates a visualization of the question tree

### 2. RAG Engine

The RAG (Retrieval Augmented Generation) engine is responsible for:
- Creating embeddings for user questions
- Retrieving relevant documents from the knowledge base
- Generating answers using LLMs (OpenAI and Anthropic)
- Including source references for verification
- Building a tree of related questions and answers

### 3. Tree Visualization

The tree visualization component:
- Renders the question tree as an interactive HTML document
- Displays the hierarchy of questions and answers
- Shows source references for each answer
- Provides a user-friendly interface for exploring the research

## Data Flow

1. User submits a question
2. Question is processed by the RAG engine
3. RAG engine retrieves relevant documents
4. LLM generates an answer with source references
5. System generates follow-up questions
6. Process repeats for each follow-up question to build the tree
7. Tree visualization is generated
8. Complete response is returned to the user

## Configuration

The system is configured through:
- Environment variables for API keys and service endpoints
- Configuration files for token limits, model parameters, and other settings
- Command-line arguments for local testing

## Development Guidelines

When working on this project:
1. Maintain the separation of concerns between components
2. Ensure proper error handling and logging
3. Follow the token limit guidelines for different tree depths
4. Always include source references in generated answers
5. Test changes locally before deploying to AWS Lambda 